\documentclass[oneside,12pt,article]{article}
\usepackage[letterpaper,top=1in, bottom=1in, left=1.5in, right=1in]{geometry}
\usepackage{blindtext}
\pagestyle{plain}
\usepackage{subfig}

\usepackage{glossaries}

\makeglossaries

\newglossaryentry{Linear Modules}
{
  name=Linear Modules,
  description={Also known as Linear Regression, it is a prediction module that uses a single variable to predict
another variable. For example, merely using age to predict the likelihood of covid-19 infection}
}

\newglossaryentry{Viral Vectors}
{
  name=Viral Vectors,
  description={Are non-pathogenic viruses used to deliver corrective genetic materials to defective cells}
}

\newglossaryentry{Genotoxic Vectors}
{
  name=Genotoxic Vectors,
  description={Are a type of viral vectors that could cause an adverse side effect }
}


\usepackage{blindtext}


\usepackage{longtable}
\usepackage{url}
\usepackage{hyperref}
 \usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}


\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},  
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,     
  breaklines=true,         
  captionpos=b,           
  keepspaces=true,         
  numbers=left,           
  numbersep=12pt,          
  showspaces=false,         
  showstringspaces=false,
  showtabs=false,          
  tabsize=2
}
\lstset{style=mystyle}
\title{Handling Overfitting in Deep Learning Models and Their Applications}

\date{\today}
\author{Isaac F. Bensaid BSc }
\begin{document}
 \pagenumbering{gobble}
 \maketitle
 \newpage
 \pagenumbering{arabic}
 \tableofcontents
 \newpage
 \section{Introduction}


Given the uncertainty of the future, decision-making is a difficult task. If we can accurately predict the future, decision-making becomes easier. While making predictions might seem easy, making accurate ones is hard. When Data Scientists attempt to predict the future, many obstacles can arise. One obstacle is Overfitting, a common problem in Machine Learning and Deep Learning. Data Scientists often rely on historical data to predict the future. For instance, assume we would like to predict the outcome of the next presidential election. There is a tremendous amount of historical data about presidential elections. The question becomes, how do we select Hyperparameters – the data that could produce the most accurate prediction results – from a large pool of historical elections data. When Data Scientists do not choose their data wisely, Overfitting could undermine their predictions' accuracy. In some cases, a high number of Hyperparameters could increase the complexity of the Learning Algorithm and increase the likelihood of Overfitting\cite{ying2019overview}. Historical data helps Data Scientists train Learning Algorithms – also referred to as Machine/Deep Learning Models. Machine/Deep Learning Model can make accurate predictions without being explicitly programmed. Data Scientists train Machine/Deep Learning Models using a set of chosen training data: a collection of situations for which the desired output is known. For example, a collection of election data in which Data Scientists know the election outcome beforehand. The goal is that the Model will also perform well on predicting the output when fed validation data – data that the Model did not encounter during its training and its output was unknown. The ability to perform well on validation datasets is called Generalization \cite{bengio2017deep}. Overfitting occurs when a model performs well on the training dataset and underperforms on the validation dataset \cite{ying2019overview}. We can explain Overfitting intuitively because all data – related to the mentioned election outcome prediction example – can be divided into two groups. The first group has data helpful for accurate election outcome prediction (also known as Hyperparameters). The second group has useless data for accurate election outcome prediction (also known as noise\cite{ying2019overview}). Machine/Deep Learning Models tend to overfit by memorizing properties of noise data that do not serve them well during the prediction phases. The higher the uncertainty and complexity for any prediction task, the more noise in its historical data. We may reduce Overfitting if we successfully determine which historical data to ignore \cite{bengio2017deep}. One of the most prominent qualities in Deep Learning/ Machine Learning is how well a model performs on unseen data. Making accurate predictions on unseen data is a fundamental element of any DL/ML Model \cite{lawrence2000overfitting}. Many Data Scientists have been attempting to reduce Overfitting to improve prediction accuracy \cite{Xu2019OverfittingRB} \cite{krizhevsky2012imagenet}. However, up until today, there is no standard solution for Overfitting. No single technique will work perfectly for every problem. Each problem required a unique solution which may include more than one technique \cite{moradi2020survey}. For example, Biologists have relied on Machine Learning to prevent or cure diseases \cite{huang2021machine}. In gene therapy — the practice of inserting corrective genetic material into defective cells — Biologists have used Machine Learning methods to develop or improve non-pathogenic viruses. Physicians use Non-pathogenic viruses to deliver corrective genetic materials to defective cells \cite{verma2000gene}. These viruses are called vectors. Biologists believe they do not cause any human diseases because they have been disabled of any pathogenic effects \cite{wang2019adeno}. During the delivery process, many hurdles can arise. For example, the immune system can destroy the delivery virus vector, preventing the delivery of the corrective genetic material. Overfitting could arise when Data Scientists predict which potential vectors are capable of evading the immune system — to deliver the corrective genes to the targeted cells. However, this thesis will focus on Overfitting in the medical diagnosis of neurodegenerative diseases. Suppose we would like to create a Model for neurodegenerative disease medical diagnosis. To create the Model, we need to train it with a medical records dataset. The complexity and uncertainty of this Model are likely to be high, increasing the likelihood of Overfitting. If we do not address Overfitting, it can arise and discredits the accuracy of our Model – which could result in unwanted prediction results. For example, Parkinson's disease (PD) is a common progressive neurodegenerative disease — characterized by motor and non-motor symptoms. While some diseases can be diagnosed with a lab test — such as cholesterol level and blood pressure measurements — Parkinson's disease cannot. Physicians rely on medical history and physical examination to diagnose Parkinson's disease. Physicians look for classic motor symptoms: resting tremor, stiffness, and slowness of movement. Additionally, vocal defections are a common symptom in the early stages of Parkinson's disease \cite{beitz2014parkinson}. cite{gunduz2019deep} trained Convolutional Neural Networks to recognize patients with such vocal defections using sets of vocal features. However, Overfitting prevented accurate classification between healthy individuals and PD patients. Luckily, they used two methods to reduce Overfitting — L2 regulations and Dropout (more on those methods later).

 
 
 \subsection{Problem Statement}
 	 	Humans can overgeneralize sometimes. For example, assume we are conducting business with an American businessman for the first time. If we find the American businessman unethical, we might inevitably claim that all American businessmen are unethical. In machine learning, a similar issue called \textbf{Overfitting} can occur if the Model does not generalize well. \textbf{Overfitting} could occur because the Model learns the noise in the training set, which impacts its performance on unseen data. The noise picked up in the training set may not apply to unseen data, leading to inaccurate predictions. \gls{Linear Modules} tend to be less likely to overfit. The more complex the Model, the more it will be prone to Overfitting. Neural network models contain multiple non-linear hidden layers, making them complex models to learn complicated relationships. Unfortunately, \textbf{Overfitting} is a common problem that prevents Neural Networks from making accurate predictions. Overfitting occurs when Convolutional Neural Networks (CNN) fails to make accurate predictions on unseen datasets. We will focus on mitigating Overfitting in CNN modules. This problem is still considered a standing problem in ML/DL\cite{Xu2019OverfittingRB}. Therefore, \cite{Xu2019OverfittingRB} imposed Regularization on the fully connected Layer(FCL) of the convolutional neural network. In a typical CNN, weights of FCLs make up most of the parameters of the network. So, first, \cite{Xu2019OverfittingRB} sparsified connections to the FCLs by applying L1 Regularization on the weights of the FCLs, then clipped small weights to zeros. Next, \cite{Xu2019OverfittingRB} also applied L2 Regularization on all weights of the CNN to keep the weights around zero. \cite{Xu2019OverfittingRB} found that the numerous parameters of FCLs are a contributor to Overfitting. The number of parameters in CNN is vast. The high number of features makes the model complex, which tends to result in \textbf{Overfitting}. In biotechnology, therapeutics should meet high safety standards during their development. Meeting safety standards requires an accurate prediction of any adverse side effects. For example, in gene therapy, predicting the safety level of \gls{Viral Vectors} could be a challenge due to Overfitting. Several individuals developed cancer during a gene therapy clinical trials using viral vectors to deliver corrective genes. In a study done by \cite{schwarzer2021predicting}, it was demonstrated that \gls{Genotoxic Vectors} have a unique gene expression signature. Based on this finding, \cite{schwarzer2021predicting} developed a "surrogate assay for genotoxicity assessment" (SAGA). SAGA uses this unique gene expression signature to distinguish genotoxic vectors from safe vectors, using machine learning to recognize the said gene expression signature. SAGA achieved an accuracy of 90.91\% on a dataset of vectors with known genotoxic potential. This level of accuracy is arguably high, but given that gene therapy could lead to cancer, methods to identify safe gene therapy vectors require a higher degree of accuracy.  
\subsection{Convolutional Neural Networks}
	Convolutional Neural Networks or CNNs are a kind of neural network. CNNs focus on data that has a gird-like representation. For example, images that we represent as a 2-D grid of pixels. CNNs employ a linear mathematical operation known as convolution. Convolution is defined as the integral of the product of two real-valued functions after one is reversed and shifted. Let $f$ and $g$ be two real-valued functions, the convolution of the two functions is written as $f * g $ , denoting the convolution operator with the symbol “$*$” \cite{al2017review}\cite{kisacanin1994fast}.




\begin{center}
	$(f*g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) \,d\tau $  

\end{center}
We could utilize several types of convolutions in ML and DL. Depending on the convolution operation, the input will be translated into some form, depending on the convolution operation used. Convolution can be detrimental to input data. For example, if we use a CNN to detect traffic lights, the bright color location is crucial. Changes to the location of the bright color can be an adverse impact caused by convolution. Pooling helps preserve the bright color in the image after the convolution layer's transformations. Pooling is a technique used to limit translation on the input data. CNN architectures can vary based on their specific application. Typically, CNNs consist of an input and output layer and several hidden layers in between. There is no one conventional CNN architecture. The type of architecture used in a CNN depends on the problem and its data. For example, It could be the case that a particular benchmark has a specific CNN architecture that is known to be the best. As scientists conduct further research, this architecture will eventually evolve\cite{bengio2017deep}. CNN can be used in the medical field to help doctors diagnose patients. In \cite{barbero2021ordinal}, CNN was used to assess neurological damage in Parkinson's disease patients using 3D brain images. \cite{barbero2021ordinal} used two CNN architectures differing on the output layer shape, the activation function, and the loss function. When using the same dataset of 508 3D brain images, it was evident that one of the different architectures outperformed the other by achieving higher classification accuracy of Parkinson's disease patients. The difference in prediction accuracy shows how modification in CNN architecture can improve its performance.  

\subsection{What is Overfitting in ML/DL}
To demonstrate Overfitting, we used a dataset that contains medical records of Parkinson's disease patients. We programmed this experiment in Keras (Python interface for artificial neural networks \cite{ketkar2017introduction}). The dataset we used contains two sub-directories. The first sub-directory has a collection of wave drawings of healthy patients, while the second sub-directory has a collection of wave drawings of patients with Parkinson's disease. The dataset is available \cite{basel99_2021} \href{https://www.kaggle.com/basel99/parkinson-s-disease-detection/data}{here.} 
 

\begin{center}
\includegraphics[width= 10cm,]{/Users/khalil/Desktop/Thesis/image/ParkinsonDisease.png} 
\newline
Figure 1: Waves drawings of healthy patients and patients with Parkinson's disease. 
\end{center}
Figure 1 shows the waves drawings from the Parkinson's disease dataset. We randomly picked images from both sub-directories. Motor symptoms such as speed of drawing and pen pressure make detecting Parkinson's disease possible. The dataset consists of 102 images. We used 72 images to train a CNN model and 30 images to test it. Due to the small size of the dataset, the CNN model is highly prone to Overfitting. Therefore, data Augmentation — creating more data — is needed in this situation to reduce Overfitting.


\subsubsection{Experiment Results To Reduce Overfitting Using Parkinson's Disease Dataset }
We built a CNN model with four layers to classify the Parkinson's Disease data into the sub-directories mentioned above: (Health patients, patients with Parkinson's disease ). Figure 2 shows the specification of the CNN Model. 
\begin{center}
\includegraphics[width= 7cm,]{/Users/khalil/Desktop/Thesis/image/CNN_ParkinsonDisease.png} 
 \begin{center}
Figure 2 CNN Configuration
\end{center} 
\end{center} 
As shown in figure 2, images were rescaled to 128 by 128 for better resolution, and we used four CNN layers. One of size 128, one of size 64, and two of size 32. To reduce Overfitting and improve the Model's accuracy, we used the following overfitting techniques: Dropout Regularization, Weight Constraints, Weight Initialization, L2 Regularization. Due to the dataset's limited size, we generated an additional 5110 images using data augmentation. We rotated the original image by 360 degrees, and we flipped it virtually and horizontally to produce other images from the original images in the dataset.

\begin{center}
\includegraphics[width= 15cm,]{/Users/khalil/Desktop/Thesis/image/Figure07.png} 
\newline Figure 3
\end{center} 
Figure 3 shows the model performance using Weight Constraints and Dropout Regularization. The graph on the left in section (a) shows the training accuracy in blue and the validation accuracy in orange. The graph on the right in section (b) shows the training loss in blue and validation loss in orange. The same is true in section (b). 




 \begin{center}
\includegraphics[width= 15cm,]{/Users/khalil/Desktop/Thesis/image/Figure 8.png} 
\newline Figure 4 
\end{center} 
Figure 4 shows the model performance using L2 Regularization and Weight Initialization. The graph on the left in section (a) shows the training accuracy in blue and the validation accuracy in orange. The graph on the right in section (b) shows the training loss in blue and validation loss in orange. The same is true in section (b). 


\begin{center}
\begin{tabular}{ |c|c|} 
 \hline
 Method & Accuracy \\ 
 Dropout Regularization & 57.41 \% \\ 
 L2 regularization & 68.37 \% \\ 
 Weight Constraints & 60.71 \%\\ 
 Weight Initialization & 69.57 \% \\ 
 \hline
\end{tabular}
\begin{center}
 Table 1
 \end{center}
\end{center}
As we can see from table 1, figure 3, and figure 4 above. The module performed better when using Weight Initialization and L2 Regularization than Weight Constraints and Dropout Regularization.
 
 \subsubsection{Experiments Discussion }
Overfitting takes place when the margin (gap) between the training error and test error is large \cite{bengio2017deep}. The models have overfitted the data, as shown in the plots above. Training accuracy and validation accuracy are off by a large margin—the lower the loss, the higher the accuracy. As we can see in some of the graphs above, the training set has a low loss, which yields higher accuracy, but the validation set has a high loss which yields low accuracy. A high difference between training and validation loss is a sign of Overfitting. In other words, the models performed well in the training set but performed poorly in the validation set, resulting in Overfitting. While we tried to reduce Overfitting by using a few methods. Such as Dropout Regularization, Weight Constraints, Weight Initialization, L2 Regularization. Based on the figures above, there is still room for improvement. We saw a slight reduction in Overfitting after applying Weight Initialization and L2 Regularization on the Parkinson's Disease dataset because the training and validation accuracy are closely aligned. Due to Random Access Memory and GPU limitations, we could not produce a more significant number of augmented data and train the models for a longer time. If we train the Model with more data and longer time, there may be room for improvement, with a higher RAM and GPU.  



\section{Methods}
Suppose we would like to create a model to detect objects. To create the Model, we need to train it with a dataset that contains images of the object of interest. Depending on the complexity of the Model, Overfitting can arise and deters the Model from correctly detecting the object of interest. There has been many research and development regarding Overfitting to improve the detection accuracy of such models. Several methods that help to reduce Overfitting have been developed \cite{bengio2017deep}\cite{ying2019overview}, such as Regularization, early stoping, Dropout, Expanding the training set as well as the methods mentioned in this section. Additionally, we can be interested in understanding how learning, cognition, and creative behaviors emerge \cite{yin2020network}. To conduct an artificial intelligence-driven study, we may use microscopic brain images, such as (Human brain MRI, Electrocorticography, Gene Atlas, and Tracer Injection). Any model we create using the said data will be complex and depend on several hyperparameters. The proliferation of hyperparameters will most likely lead to Overfitting. 
\subsection{Regularization}
Generally, any element added to the prediction model to compensate for data-scarce is referred to as regularization \cite{moradi2020survey}. Regularization is one form of regression that discourages learning a more complex model to avoid the risk of Overfitting. We will start with a simple introduction to regression and more deep learning-related regression methods to motivate Regularization. The main objective of regression is to calculate the prediction error and minimize it. Predicting error and minimizing it can happen using several strategies. We can do so by altering the Model's capacity: A model's capacity can fit a wide variety of functions. Models with high capacity can learn from complex data and produce accurate results. It was concluded in \cite{wang2017growing} that increasing CNN models' capacity helped its units better adapt and specialize to its tasks \cite{bengio2017deep}. For example, in a linear regression problem, our hypothesis space or capacity is the set of all linear functions. We can increase our capacity by adding the set of all polynomials to our hypothesis space. This increase will most certainly lead to Overfitting because our Model will have many functions to fit our data. The right size of capacity depends chiefly on the problem's complexity. Simple regression has a single explanatory variable \cite{sykes1993introduction}. It means that we will use a single variable to predict another variable. For example, If we merely used age to predict the likelihood of Parkinson's Disease. In reality, any effort to make predictions using a single variable without attention to the other factors could result in statistical difficulties (termed "omitted variables bias"). For example, we say that age increases the likelihood of Parkinson's Disease without looking into other factors. 

 \begin{center}
\includegraphics[width= 13cm,]{/Users/khalil/Desktop/Thesis/image/regression01.png} 
\newline
\begin{center}
Figure 5: This data generated randomly for illustration. 
\end{center}
\end{center}

Take figure 5, for example. We can see that sometimes older people tend to develop Parkinson's disease. However, the relationship is not perfect. It seems that age does not suffice for an accurate prediction of the likelihood of Parkinson's disease. Therefore, we may deduce that other factors could influence the likelihood of Parkinson's disease; we may refer to them as "noise". Nevertheless, we can write the above relation mathematically as follows:
   
 \begin{center}
$ CI = 	 A\beta + 	\epsilon $,
\end{center}
Where $\beta$ is the effect of an additional year of age on the likelihood of Parkinson's disease and $\epsilon$ is the "noise". CI is the dependent variable; A is the independent variable. The task of regression is to produce an estimate of  $\beta$, based on the data provided and taken $\epsilon$ into account, meaning that we need to calculate the error $\epsilon$ and minimize it. One way to calculate the error is to calculate the difference between the actual and predicted values and average them. However, this might be problematic since there is a chance that the averaged variable will cancel each other out. 
\newpage
For this reason, we take the sum of the Square, formally known as Residual Sum of Squares (RSS)\cite{archdeacon1994correlation}:  

 \begin{center}
$ RSS = \sum_{i=1}^{n} (y_{i}-f(x_{i}))^{2} $,
\end{center}
Since this is a relatively simple example, we can shift our attention to more deep learning methods to minimize error. We will start with Dropout and move to Data augmentation, Ridge Regularization, Bayesian Regularization, and Early Stopping. 

\subsubsection{Dropout}

Combining two different models help with reducing Overfitting via averaging their prediction. Unfortunately, such a process can be prolonged. Dropout helps to finesse this situation. Dropout is a simple method that prevents Neural Networks from Overfitting; when we say Dropout, we mean dropping units from a neural network, temporarily along with their associated connections, as shown in Figure 6.

\begin{center}
\includegraphics[width=\linewidth]{/Users/khalil/Desktop/Thesis/image/figure1.png}
Figure 6: left diagram shows the neural network before applying Dropout \cite{srivastava2014dropout}. 
\end{center}
The choice of which units to drop is random. Units that survived Dropout will constitute a "thinned" neural network. The left diagram in Figure 6 shows a thinned neural network. In \cite{srivastava2014dropout}\cite{bengio2017deep} it was shown that classification using Dropout is better than 1000 sub-networks using Monte Carlo approximation. In \cite{srivastava2014dropout} it was also shown that it is better than L1 and L2 Regularization. The computation and memory complexities of Dropout are ideal at O(n) – which is a function that increases linearly to reflect the time and space complexity of Dropout \cite{moradi2020survey}. A linear complexity – as if the running time of a given algorithm increases linearly with the algorithm input – is highly preferable over higher-order functions/time complexities. For example, some algorithms have a quadratic time complexity typically denoted as $O(n^2)$. While Dropout ameliorates Overfitting, it also helps to combine enormous neural network architectures sufficiently \cite{srivastava2014dropout}.  

\subsubsection {Training with More Data - Data augmentation} 

The quantity and quality of the training dataset are a big factor in the Model's overall performance and accuracy. Expanding the training dataset will help improve the performance and accuracy of the Model. If we add noisy data, this technique will not work. However, training with more data can help improve the accuracy of the Model, especially in complicated models \cite{ying2019overview}. There is, however, a downside to this method. The high amount of data will increase the training time. Training data could be expensive to obtain. In the experiment done in \cite{krizhevsky2012imagenet} to combat overfitting in the ImageNet dataset – a dataset with 15 million labeled images mapped to 22,000 categories – \cite{krizhevsky2012imagenet} used data augmentation. First, \cite{krizhevsky2012imagenet} used "label-preserving transformation," which added more data to the training set. \cite{krizhevsky2012imagenet} extracted random 224$\times$224 patches from the 256$\times$256 images and trained the CNN merely on the extracted patches. The enlarging of the training set allowed for a more complex model without suffering from substantial Overfitting. Second, \cite{krizhevsky2012imagenet} performed Principal Component Analysis (PCA) – the method used to reduce the dimensionality of large datasets while preserving most of its information \cite{jolliffe2016principal} – on the set of RGB (stands for Red Green Blue: combining these colors can produce images on screens ) pixel values through the training set and alter the intensities of the RGB channels in training images. To each training image, \cite{krizhevsky2012imagenet} adds multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel. $ I_{xy} = [I^{R}_{xy}, I^{G}_{xy}, I^{B}_{xy}]^T $ \cite{krizhevsky2012imagenet} add the following quantity:

\begin{center}

$[P_ 1, P_2, P_3] [\alpha_1 \lambda_1 , \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T$

\end{center}
where $\alpha_i$ and $\lambda_i$ are ith eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and $\alpha_i$ is the aforementioned random variable \cite{krizhevsky2012imagenet}. Each $\alpha_i$ is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is redrawn. This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. This scheme reduces the top-1 error rate by over 1 \%.


\subsubsection{Weight Decay known as (L2 or ridge regularization)}
We only mentioned one way to control a model's capacity. Instead of excluding or including more functions in our hypothesis space, we can also alter our capacity by expressing a preference for a specific function in the hypothesis space. This can be done by adding a parameter norm penalty $\Omega (\theta)$ to the objective function J \cite{bengio2017deep} \cite{meng2018enhancement}:

 \begin{center}
$ \hat{J} (\theta; X,y) = J(\theta; X,y) + \alpha \Omega (\theta) $ ,
\end{center}
Where $\alpha $ is a hyperparameter that weights the relative contribution of the norm penalty term $\Omega$ relative to the standard objective function $J(x,\theta)$. Setting $\alpha $ to 0 results in no regularization. Larger values of $\alpha $ correspond to more regularization \cite{bengio2017deep} \cite{meng2018enhancement}. As was shown in \cite{meng2018enhancement} picking different parameter norms $\Omega$ leads to different preferred solutions that suits the problem in question. In \cite{meng2018enhancement}: 

 \begin{center}
$ \hat{J} (\omega; X,y) = \frac{\alpha_1}{2}\omega^T \omega +\alpha_2 | \omega |_1  +J(\omega; X,y) $ ,
\end{center}
was used as objective function J. With $ \frac{\alpha_1}{2}\omega^T \omega +\alpha_2 | \omega |_1 $ as a parameter norm penalty they were able to reduce overfitting and produce a state of the art results for denoising autoencoders: a special type of neural network which takes corrupted data as input and predict the uncorrupted data. But in ridge regularization to reduce variance and generalization error – the proportion of incorrect output – in a DL model, researchers added: 
 \begin{center}
 $\Omega (\theta) = \frac{1}{2} \Vert w \Vert^{2}_{2}$ ,
\end{center}
as parameter norm penalty to the objective function. This term is known as weight decay – also referred to as Tikhonov's regularization – that could also be added to the cost function in order to restrict the model's parameters from increasing and/or penalizes the parameters of the DL model which results in a smaller squared L2 norm weight \cite{kumarstudying} \cite{tikhonov1943stability}. 


\subsubsection{Bayesian Regularization}

Bayesian Regularization imposes prior distributions – distributions that express a prior opinion before taking any facts into account – on a model's parameters and penalizes its weights \cite{okut2016bayesian}. However, typical neural network models use the mean square errors (MSE):

 \begin{center}
 $MSE= \frac{1}{n} \sum_{i=1}^{n}(t_i - a_i)^2$,
\end{center}
as an objective function and don't optimize the weights of the model \cite{sariev2020bayesian}. In typical neural networks models optimal configuration of its weights is derived by minimizing MSE. Bayesian regularization neural networks apply a probability distribution to the weights of the Model. The objective function in Bayesian regularization includes network weights. It is written as \cite{sariev2020bayesian}: 

 \begin{center}
 $F(\omega)= \alpha E_\omega + \beta E_D $,
\end{center}
Where $ E_D $ is MSE. $\beta$ and $\alpha$ are objective parameters. $ E_\omega $ is the sum of squared network weights also known as weight decay:
 \begin{center}
 $E_\omega = \frac{1}{n} \sum_{i=1}^{n}(w_j)^2 $.
\end{center}
A gradient-based based optimization method is used to minimize the function $F(\omega)$ and optimize the hyperparameters $\beta$ and $\alpha$. Here is an overview of this method. For more details see \cite{yue2011bayesian}\cite{mackay1992practical}. Given a M as a neural network model, $\omega$ as vector weights and N as total number of network weights. $P(\omega|,\beta,M) $ is the prior density. $P(D|\omega, \beta, M)$ is the likelihood function. $Z_D(\beta)$ , $Z_\omega(\beta)$ and $P(D|\alpha, \beta,M) $ are the normalization factors. Based on Bayesian rule we have:  

 \begin{center}
$ P(\omega| D,\alpha, \beta,M) = \frac{P(D|\omega,\beta,M) P(\omega|\alpha,M) }{P(D|\alpha, \beta,M) } $
\end{center}
Since the prior distribution of network weight is viewed as a distribution of Gaussian, we have \cite{kayri2016predictive}: 

 \begin{center}
$ P(D|\omega,\beta,M) = \frac{e^{-\beta E_D} }{Z_D(\beta) } $
\end{center}

 \begin{center}
$ P(\omega|\alpha,M) =\frac{e^{-\alpha E_\omega}}{Z_\omega(\alpha)} $
\end{center}

 \begin{center}
$ Z_D(\beta) =(\frac{\pi}{\beta})^\frac{n}{2}$
\end{center}

 \begin{center}
$ Z_\omega(\alpha)=(\frac{\pi}{\alpha})^\frac{N}{2} $
\end{center}
Now the optimal weight should maximize the posterior probability $ P(\omega| D,\alpha, \beta,M) $ \cite{yue2011bayesian}. \cite{pan2020inverse} showed that Bayesian Regularization reduced Overfitting in inverse modeling for filters using Deep Neural Network. Moreover, \cite{pan2020inverse} increased the prediction accuracy of the Neural Network by using Bayesian Regularization when trying to predict coupling Matrix from Simulated S-Parameters.  


\subsubsection{L1 Regularization}

As \cite{ ying2019overview} summarized it, for neural networks, the objective is to find a perfect set of weights and biases. Finding an ideal set of weights and biases becomes complicated when the number of hyperparameters increases. To find an ideal set of weights and biases, the Model needs sufficient data for learning. The amount of data needs to be proportional to the number of hyperparameters. The Model's complexity increases as the number of hyperparameters increases. Even though some do not affect its accuracy, overfitting models consider all hyperparameters. So, we need to minimize the weights of hyperparameters that do not affect the Model's accuracy. Because we do not know which hyperparameters affect the Model's accuracy, L1 Regularization tries to limit them all by minimizing the cost function of the Model. L1 Regularization adds a "penalty term" to the cost function as shown in the following formula:

 \begin{center}
 $\Omega (\omega) = \Vert w \Vert_1 = \sum_{i} \Vert w_i \Vert$ ,
\end{center}



\subsubsection{Using Multiple Loss Functions}
\cite{xu2015multi} used a DNN model with multiple loss functions to reduce Overfitting. \cite{xu2015multi} noted that researchers have not well explored the optimization of loss functions to prevent the overfitting problem. \cite{xu2015multi} used four kinds of the loss function in one Model, softmax loss, pairwise loss, LambdaRank top-1 loss, and LambdaRank top-2 loss. \cite{xu2015multi} explained that other loss functions might have the potential to prevent the algorithm overfitting to one softmax loss function. \cite{xu2015multi} was able to achieve a state of the art results on the CIFAR-10, MNIST, and SVHN datasets.

\subsubsection{Early Stopping}
Stopping the training early before the Model overfits the training dataset can reduce Overfitting. By monitoring the generalization error of the Model – a difference between the loss/error of a training dataset and its test dataset \cite{jakubovitz2019generalization} – and training the Model multiple times with different values to select the number of epochs that produce the lowest error rate \cite{prechelt1998early}. This method can be used to detect when Overfitting starts during supervised training; training is then stopped before convergence to avoid Overfitting. This method updates the Model to make it better fit the training data with each iteration. However, improving the Model's fit to the training data may increase the generalization error. 

\begin{center}
\includegraphics[width= 10cm,]{/Users/khalil/Desktop/Thesis/image/earlystop.png} 
\newline
Figure 7 shows how training and validation errors decreased in Parkinson's Disease Dataset experiment from section 1.3. 
\end{center}When we run the Model, the training and validation errors decrease in parallel until epoch 7, then the validation error starts to increase (as we can see from figure 7). Early Stopping uses this fact to check at every iteration. If validation error is increasing, we stop the Model. In general terms, we execute the Model until the error on the validation set has plateaued; while doing so, we keep a record of the model configuration and the time the Model improved. Upon resuming training we, return to the stored configuration \cite{bengio2017deep}. In \cite{basnin2021deep} the model configuration was preserved using the so-called Checkpoint function in Keras ­– a function that saves the configuration of the Model once the early stopping algorithm halts the training. Using this approach \cite{basnin2021deep} was able to achieve high classification results for diagnosing Parkinson's disease patients using MRI images. Notably, Early Stopping is equivalent to L2 Regularization, when we have a simple linear model with a quadratic error function and simple gradient descent \cite{bengio2017deep}. This was also shown in \cite{li2020gradient} where early Stopping was resembled by using the quadratic function:
\begin{center}
$L(\Omega) = \sum_{i=1}^{n}(y_j - f(W,x_i))^2$
\end{center}
as a loss function and applying gradient descent. Gradient descent is an optimization algorithm used in deep learning models to minimize their objective function $J(\theta)$ by finding its local minimum. The algorithm starts by setting $\theta$ equal to a random value. A constant $alpha$ known as learning rate is used as a small value that determines the length of the algorithm's step towards the local minimum. Then it computes the derivative of the function $J' (\theta)$ at the given values of $\theta$. The algorithm repeats this process until it reaches a value of $\theta$ where the derivative is equal to zero. At this value of $\theta$, the function will be at its local minimum, and the function will converge \cite{bengio2017deep}\cite{ruder2016overview}.   


\subsection{Modifying The Activation Function}
This method shows the correlation between overfitting and activation functions. A modified activation function called: modified-sigmoid is based on the well-known sigmoid function. This activation
function can effectively improve the accuracy of the Model and inhibit the overfitting problem \cite{li2019research}. Before we dig deep, let us describe activation functions and their role in neural networks.  
\subsubsection{General Neural Network Architecture}
Every Neural Network contains many perceptrons. To understand how Neural Network works, we need to understand how perceptions work. Perceptrons are a function that can take any form of data as input and return a boolean value as output. A perceptron has weights, a bias, a weighted sum, input values, and an activation function.  

\begin{center}
\includegraphics[width=\linewidth]{/Users/khalil/Desktop/Thesis/image/perceptron.png}
Figure 8: An image of a single perceptron. Adapted from \cite{gankidi2017fpga}
\end{center}
As shown in Figure 8, all the inputs $x$ are multiplied with their weights $w$, then we add all of the multiplied values and call the result the Weighted Sum. The weighted sum gets passed to the activation function. 
\newline
\newline
\begin{center}
\includegraphics[width=\textwidth]{/Users/khalil/Desktop/Thesis/image/ArtificialNeuralNetwork.png}
Figure 9 shows the entire Neural Network Architecture. Adapted from \cite{gankidi2017fpga}

\end{center}
Every node in the Neural Network architecture shown in Figure 9 contains a perceptron. If we were to zoom closer at each node in the Neural Network Architecture, we would see the perceptron shown in Figure 11.    
As we can see, there are many layers to the Neural Network; the first layer is called the input layer, the last layer is called the output layer, and the layers in between are called the hidden layer. Each perceptron layer can receive input from the previous layer and produce output to the next layer. Therefore, the Neural Network Architecture can vary based on the type of Neural Network used and the problem we solve. For instance, CNN will have a slightly different architecture than what we just saw in Figure 9. 


\begin{center}
\includegraphics[width=\textwidth]{/Users/khalil/Desktop/Thesis/image/CNN.png}
Figure 10 shows a sample of CNN Architectures. Adapted from our Parkinson's Disease example in section 1.3  

\end{center}
In figure 10, we predicted whether a patient has Parkinson's disease from their hand-written wave. The image has dimensions height = 128 pixels, width = 128 pixels, and a depth of 1. Next, the image moves to the convolution layer shown in figure 15. Applying six filters of size 5$\times$5 yields an image of height = 124 pixels, width = 124 pixels, and a depth of 6. Then it gets passed to the pooling layer, applying the same amount of filters and resulting in the same depth with an image of height = 61 pixels width = 61 pixels. We repeat this process until we flatten the image to a single layer, and then we feed it to the network shown in Figure 9 to classify it into the appropriate category \cite{al2017review}.

\begin{center}
\includegraphics[width=\textwidth]{/Users/khalil/Desktop/Thesis/image/Conv.png}
Figure 11 shows a breakdown of the convolution layer.  
\end{center}
For simplicity, we will assume that the wave image is in 5$\times$5 format. Applying a 3$\times$3 filter will result in a 3$\times$3 image. Every value in the filter is multiplied by its corresponding value in the original image. With Striding —the amount of movement over the image — equals one, the filter moves one step to the right and repeats the same process \cite{al2017review} \cite{li2019deep}. 


\begin{center}
\includegraphics[width=10cm]{/Users/khalil/Desktop/Thesis/image/Pooling.png}
\newline
Figure 12 shows a breakdown of the pooling layer.  
\end{center}
Here we take a look at the pooling layer. Simply put, we take a small portion of the image, and we apply either max-pooling — taking the maximum number of the portion we select – or average pooling, taking the average of the portion we select and rounding it up \cite{al2017review} \cite{li2019deep}. Now that we have a basic visualization of the Neural Network Architectures, we can dig deep into activation functions.  
\newpage
\subsubsection{Activation Functions}
Each layer of the neural network needs a non-linear function called the activation function before generating the output signal. When there is no non-linear activation function, this neural network layer is transformed linearly. No matter how many layers the network contains, the final output can be expressed by the linear transformation of the input. Therefore, it has no difference from the single-layer neural network \cite{li2019research}. There are three commonly used activation functions: 
\newline
\newline
Sigmoid: Sigmoid activation function:

\begin{gather*}
   Sigmoid = \frac{1}{1+ e^{-x}} 
\end{gather*}
Tanh activation function:

\begin{gather*}
   Tanh(x) = \frac{1- e^{-2x}}{1+ e^{-2x}} 
\end{gather*}
ReLu activation function:

 \[ReLu(x) = \begin{cases} 
   x & x > 0 \\
   0 & x\leq 0\\
  
  \end{cases}
\]
The proposed method in \cite{li2019research} introduced a new activation function that showed promising results in terms of reducing Overfitting. The sigmoid activation function is a near S-shaped curve. The modified-sigmoid activation function is an improvement of the Sigmoid activation function. By defining a hyperparameter, the Modified-sigmoid activation function is shown as follows:

 \[ModifiedSigmoid(x) = \begin{cases} 
   x  , & -w \leq x \leq w \\
   Sigmoid  , & x< -w $ or $ x>w\\
  
  \end{cases}
\]
The four different activation functions were compared by \cite{li2019research} using (MNIST) dataset for for training and testing the model in this proposed method. \newline
\begin{center}
\includegraphics[width= 15cm,]{/Users/khalil/Desktop/Thesis/image/ModifiedSigmoid.png} 

Figure 12: Effect of activation function on model accuracy \cite{li2019research}.
\end{center}

As we can see in figure 12, the activation function plays a vital role in preventing the Model from Overfitting.
After optimization, it was found in \cite{li2019research} that the neural network possesses the best test accuracy with hyperparameter $w = 2$. Furthermore, the results in \cite{li2019research} show that the Modified-sigmoid function can effectively improve the accuracy of the neural network and prevent the Overfitting of the neural network.

 \subsection{K-fold Cross-Validation }
 
 K refers to the number of groups that a given dataset will be split into. "Fold" refers to the number of resulting subsets. Cross-validation helps estimate how the Model is expected to perform when making predictions on unseen data. The general idea is to shuffle the dataset randomly and split it into k groups. We take each group as a test dataset and the remaining groups as the training set. We fit the Model on the training set and evaluate it on the test set. Once we retain the evaluation score, we discard the model \cite{ng1997preventing}. This method reduces bias and computation time as we repeat the process only ten times when the value of k is 10. Every data point is tested precisely once and is used in training k-1 times. As we increase k, the variance of the resulting estimate is reduced as well. 
 



\subsection{Removing Features}
Unjustifiable features that do not fit the Model should be removed. Some algorithms remove irrelevant features by default. Removing irrelevant input costs less computational power. This method improves prediction accuracy by the exclusion of irrelevant variables. The Model to be built is simpler and faster when fewer input variables are used \cite{reunanen2003overfitting}.\cite{yoo2019deep} experimented if deep learning can extract latent Multiple Sclerosis (MS) ¬– a neurological disease — lesion (damaged areas of the brain) features that when combined with three user-defined MRI measurements (T2w lesion volume, brain volume, and diffusely abnormal white matter DAWM) and eight user-defined clinical measurements (gender, cerebrum, optic nerve, cerebellum, brain stem, spinal cord, EDDS, and cistype) can accurately predict the chance of conversion from clinically isolated syndrome (CIS), an initial stage of MS, to clinically definite MS (CDMS). \cite{yoo2019deep} used a CNN model fed with lesion masks segmented from MRI images. \cite{yoo2019deep} applied the feature selection method to increase the accuracy of the Model and avoid Overfitting. \cite{yoo2019deep} computed the relative importance of each of the 11 user-defined features for classification between converters and non-converters by permuting the features among the training data and computing the average generalization error. They then selected discriminative features by choosing the features whose relative importance is larger than the median importance. The importance of each feature is shown in figure 13. 




\begin{center}
\includegraphics[width= 15cm,]{/Users/khalil/Desktop/Thesis/image/features MS.png} 

Figure 13: the importance of each feature on the Model's accuracy \cite{yoo2019deep}.
\end{center}
The selected features were BOD, BPF, EDSS, gender, spinal cord, and cerebellum. \cite{yoo2019deep} trained the Model using the selected features only and were able to increase the Model's accuracy from 63.6 to 73.8, which amounts to a 10 percent increase. \cite{ayinde2019regularizing} observed that neural networks have many redundant features that are very similar. \cite{ayinde2019regularizing} proposed an algorithm to regularize related features and therefore encourage features diversity in the model. As a result, \cite{ayinde2019regularizing} eliminated redundant features. This method reduced the computational overhead. \cite{ayinde2019regularizing} was able to achieve a state of the art accuracy and reduce computational overhead as a result of this method

\begin{center}
\includegraphics[width= 10cm,]{/Users/khalil/Desktop/Thesis/image/equation.png} 
\end{center}
To encourage diversity of features, \cite{ayinde2019regularizing} added the $J_D$ term to the loss function $J (\theta; X, y)$. \cite{ayinde2019regularizing} was able to achieve a state of the art accuracy and reduce computational overhead as a result of this method.





\subsection{Critical Evaluation of Overfitting Methods using Brain Data}
Now that we have discussed some methods to address Overfitting, we will implement them and discuss their advantages and drawbacks. As such, we chose a dataset that contains images of MRI Segmentation. The dataset contains images of healthy brains and brains affected by Alzheimer's — a type of dementia that affects brain functions and interferes with regular daily activities. Accordingly, we split the data into three sets. A training set to train our Model, testing set to test our Model once the training is complete, and a validation set to test our Model during the training phase and help it find the weights that can produce the highest level of accuracy. The dataset has four classes; "Non-Demented," "Mild Demented," "Moderate Demented," and "Very Mild Demented." Each of these classes represents a stage of Alzheimer's. Early detection of Alzheimer's is critical because treating it at an early stage is much easier than at advanced stages.  
 \begin{center}
\includegraphics[width= 10cm,]{/Users/khalil/Desktop/Thesis/image/AlzheimerMRI.png} 
\newline
Figure 14 MRI Images
\end{center}
We built a basic CNN model with four convolution layers, four max-pooling layers, and two dense layers. We illustrate the model specification in figure 15.  
 \begin{center}
\includegraphics[width= 10cm,]{/Users/khalil/Desktop/Thesis/image/CNN_Alzheimer.png} 
\newline
Figure 15 CNN Configuration
\end{center}
The Model was only able to achieve 55 \% accuracy. Additionally, as shown in Figure 16, the training accuracy kept increasing while the testing accuracy pleated around 50\%. Therefore, the Model is experiencing Overfitting. Consequently, we will use the methods discussed earlier to improve the Model's accuracy and reduce Overfitting.  


 \begin{center}
\includegraphics[width= 9cm,]{/Users/khalil/Desktop/Thesis/image/AlzheimerBasicModel.png} 
\newline
Figure 16 Training vs. Testing accuracy basic CNN model 
\end{center}



\subsubsection{Data Augmentation and Dropout}
Originally the training dataset had a total of 5121 images. 717 MildDemented, 52 ModerateDemented, 2560 NonDemented and 1792 VeryMildDemented. On the other hand, the testing dataset had 1279 images. 179 MildDemented, 12 ModerateDemented, 640 NonDemented and 488 VeryMildDemented. As shown in \cite{8261460} the left and right regions of the brain are symmetrical. We augmented the data by flipping the image along the horizontal axis. Due to Random Access Memory limitation, we could only produce 77126 images for the training set and 20601 images for the testing set. We tried to balance the dataset by producing more images in the classes with a limited number of images. For example, we only had 12 MildDemented images in the training set. As such, we produced 17925 images of the MildDemented class. Finally, we added two layers of Dropout, each with a Dropout rate of 0.5. 


 \begin{center}
\includegraphics[width= 9cm,]{/Users/khalil/Desktop/Thesis/image/AlzheimerDataAugAndDropOutModel.png} 
\newline
Figure 17 Training vs. Testing accuracy Data Augmentation and Dropout CNN model 
\end{center}
We were able to increase the Model's accuracy by 4\%. After applying the mentioned Dropout and Data Augmentation techniques, the Model achieved a 59\% accuracy. However, as shown in figure 17, the Model is still experiencing Overfitting as the testing accuracy plateaued and the training accuracy increased after reaching roughly Epoch 25.  

\subsubsection{Data Augmentation and L2 Regularization with and without Dropout }
In this section, we trained the Model with Data Augmentation in addition to L2 Regularization. Then, we trained the Model again with Data Augmentation, L2 Regularization, and Dropout regularization.  




 \begin{center}
\includegraphics[width= 9cm,]{/Users/khalil/Desktop/Thesis/image/AlzheimerDataAugL2.png} 
\newline
Figure 18 Training vs. Testing accuracy Data Augmentation and L2 Regularization without Dropout CNN model 
\end{center}
When training the Model with Data Augmentation and L2 Regularization without Dropout regularization, the Model achieved 57\% accuracy. However, compared to training with Data Augmentation and Dropout Regularization, the Model underperformed by 2\%. We previously performed 59\% accuracy with Data Augmentation and Dropout regularization. As shown in figure 18, the Model is still undergoing Overfitting as the training accuracy is plateauing at around 90\%, and the testing accuracy is plateauing around 57\%.


 \begin{center}
\includegraphics[width= 9cm,]{/Users/khalil/Desktop/Thesis/image/AlzheimerDataAugAndDropOutModelwithL2.png} 
\newline
Figure 19 Training vs. Testing accuracy Data Augmentation and L2 Regularization with Dropout CNN model 
\end{center}
When training the Model with Data Augmentation and L2 Regularization with Dropout regularization, the Model achieved 62\% accuracy. Nevertheless, compared to training with Data Augmentation and Dropout regularization, the Model overperformed by 3\%. We previously performed 59\% accuracy with Data Augmentation and Dropout regularization. Additionally, the Model overperformed by 5\% compared to the Model we trained with merely Data Augmentation and L2 regularization. As shown in figure 19, the Model is still undergoing Overfitting as the training accuracy is plateauing at around 90\%, and the testing accuracy is plateauing around 62\%.


\subsubsection{Early Stopping with Data Augmentation, L2 and Dropout Regularization}
In the previous models, we set a specific number of Epoch — the number of passes of the entire training dataset the Model has completed. The Model had to go through the whole training set based on a pre-specified number of epochs regardless of whether its performance improved. Therefore, training without Early Stopping took a long time and did not improve the Model's accuracy. We utilized the Early Stopping method by monitoring the Model's accuracy to save time. Since we are dealing with a classification problem, monitoring the accuracy would be most appropriate instead of monitoring the loss. Once we noticed an improvement in training accuracy, we saved the Model's configuration and returned to it for further training. We halted the training once we noticed that the Model's accuracy had not improved for a long time. We pre-specified how many epochs the Model should go through before we halt the training once we notice no improvement in accuracy. Table 2 shows how the Model improved throughout the training process. Once the Model's accuracy reached 0.63880, it stopped improving for 87 epochs, and that was where we decided to halt the training. The Model may improve if we let it go beyond 87 epochs at the expense of time. 


\begin{center}
\begin{tabular}{ |c|c|} 
 \hline
 From & To \\ 
 -inf & 0.52968 \\
0.52968 & 0.57099 \\ 
0.57099 & 0.57366 \\ 
0.57366 & 0.57832 \\  
0.57832 & 0.58784 \\ 
0.58784 & 0.58798 \\ 
0.58798 & 0.59177 \\ 
0.59177 & 0.60293 \\ 
0.60293 & 0.60881 \\ 
0.60881 & 0.60973 \\ 
0.60973 & 0.63060 \\ 
0.63060 & 0.63278 \\ 
0.63278 & 0.63613 \\ 
0.63613 & 0.63880 \\ 
 \hline
\end{tabular}
\begin{center}
 Table 2
 \end{center}
\end{center}
Overall the Model achieved an accuracy of approximately 64\%, Which is an improvement from the previous 62\% achieved by the Model trained with Data Augmentation, L2, and Dropout regularization. 

\subsubsection{Changing the Activation Function and Removing Features }
One irrelevant feature that we could have removed to improve the Model's accuracy is the skull from the MRI image. However, as shown in figure 19, the skull was already removed from the dataset. We are currently not aware of any other irrelevant features that we could remove to improve the Model's accuracy. 


 \begin{center}
\includegraphics[width= 9cm,]{/Users/khalil/Desktop/Thesis/image/Alz.png} 
\newline
Figure 20 Brain MRI image extracted from the Alzheimer training set. 
\end{center}
As shown in Table 3, we trained the Early Stopping with Data Augmentation, L2, and Dropout Regularization model with several different activation functions. The Model improved when using the Hyperbolic Tangent Activation Function, achieving a 66\% classification accuracy. However, the accuracy worsened when using other activation functions. 

\begin{center}
\begin{tabular}{ |c|c|} 
\hline
Activation Function & Accuracy \\ 
Rectified Linear Unit & 63\% \\
Sigmoid & 53\% \\ 
Hyperbolic Tangent & 66\% \\ 
Leaky version of a Rectified Linear Unit & 62\% \\  
Parametric Rectified Linear Unit & 61\% \\ 
Thresholded Rectified Linear Unit & 52\% \\ 
Softmax & 53\% \\
 

\hline
\end{tabular}
\begin{center}
 Table 3
 \end{center}
\end{center}

\subsubsection{Discussion}

We reached the highest accuracy when we trained the Model with Early Stopping, Data Augmentation, L2, and Dropout Regularization using the Hyperbolic Tangent Activation Function. One drawback we noticed when using Dropout is a longer training time than training the Model without Dropout. As \cite{poernomo2018biased} stated, training a model with Dropout Regularization will significantly increase the training time. Models trained with Dropout Regularization take a long time than models trained without Dropout Regularization. Dropout creates thinned networks after randomly dropping several units and combines them all together. As stated in \cite{poernomo2018biased} training a Dropout neural network with n hidden units can be seen as training a collection of 2n different "thinned" models with extensive weight sharing. \cite{poernomo2018biased} improved on the traditional Dropout method by omitting to drop units from the neural network that has a positive impact on the Model's accuracy and opting to dropout units that have no potential to improve the Model's accuracy. Units with high activation values positively impact the Model's accuracy, and units with low activation values have a low impact on the Model's accuracy. Selectively adjusting the amount of Regularization on the Model's weights has increased the accuracy of neural networks by reducing Overfitting. \cite{ kirkpatrick2017overcoming} showed that it could also mitigate catastrophic forgetting. Catastrophic forgetting occurs when a network learns task A and forgets it once it starts to learn another task B. Catastrophic forgetting occurs because the weights in the network that are important for task A are changed to meet the objectives of task B. To mitigate catastrophic forgetting \cite{ kirkpatrick2017overcoming} reduced Regularization on certain weights based on how important they are to previously seen tasks. Similarly, \cite{khan2019regularization} put more weight on the unit outputs that support correct predictions on the training set when they introduced a novel dropout regularization called Spectral Dropout (SD). SD discarded noisy spectral components during the train and test phases. The spectral Dropout speeds up the convergence rate during the training process. As stated in \cite{ma2019transformed}, to reduce computation time, many network weights are redundant, and a regularizer can remove it from the network without much loss of performance. Different from the traditional Dropout method, with the moving of convolution filters on the input feature maps, \cite{pan2020dropfilterr} dropped out different units with fixed or random drop rates, introducing more uncertainty. However, experimental results indicated that \cite{pan2020dropfilterr}’s method could prevent overfitting. \cite{ma2019transformed} used a spare ℓ1 regularizer and applied it to the matrix space of network weight to zero redundant weights and removed their unnecessary connections and neurons. \cite{ma2019transformed} hypothesized that reducing the number of unnecessary connections can reduce computation and memory requirements. \cite{ kirkpatrick2017overcoming} tested this method on several datasets and was able to obtain state-of-the-art accuracy. We started with 55\% accuracy with a basic CNN model, which did not use any methods to mitigate Overfitting. We ended with a 66\% accuracy with the Model that used Early Stopping with Data Augmentation, L2, and Dropout Regularization using the Hyperbolic Tangent Activation Function. Given the high complexity of the task and dataset complexity, an 11\% increase in accuracy is considered a good improvement. However, there are still signs of Overfitting, and the Model's accuracy can be improved further. Our model is very complex compared to the one used in \cite{peng2021accurate} to predict brain age. \cite{peng2021accurate} used a lightweight CNN architecture. \cite{peng2021accurate} kept only one convolutional layer before each MaxPool layer to reduce memory requirements. In addition, \cite{peng2021accurate} removed all the fully connected layers, which not only greatly reduces the number of parameters. \cite{peng2021accurate} reduced the GPU memory consumption by liming the channel numbers of the first layer to 32. We could use this idea in our research to reduce GPU memory consumption. A problem we faced when training our model on the Alzheimer dataset. Using data augmentation and regularization techniques, \cite{peng2021accurate} achieved state-of-the-art results. \cite{peng2021accurate} stated that it was intriguing why the lightweight model performs better than the deeper ones, something we can explore in our feature research. \cite{peng2021accurate} noted that the choice of optimizer might affect the model performance. \cite{peng2021accurate} noticed a change in performance when switching from Adam optimizer to Stochastic gradient descent Optimizer. As \cite{williams2020paired} concluded, the methods we mentioned and experimented with have produced state-of-the-art results in many fields; they have not gained as much ground over human neuroscience datasets. The high dimensionality and large amounts of noise present in neuroscience datasets, coupled with limited data available that can be reasonably obtained from a sample of human subjects, lead to difficulty training deep learning models. Overfitting can occur despite the presence of regularization techniques. \cite{williams2020paired} introduced the paired trial classification (PTC) method for classifications. PTC reduces the multiclass classification problem to two classes, potentially simplifying the problems and thus presenting a way of making it easier to achieve robust classification performance from limited training data. Instead of classifying a single example at a time into several classes, \cite{williams2020paired} instead attempt to classify pairs of examples as belonging to either the same class or different classes. \cite{abrol2021deep} demonstrate that DL methods, if implemented and trained according to the common practices, have the potential to outperform standard machine learning methods with lower computational time and space despite being more complex in their architecture and parameterization, using raw or minimally preprocessed input data. \cite{abrol2021deep}  states that DL approaches are just beginning to show successes in diagnostic classification and disease prediction domains.


\section{Development of DL Computational Models Addressing Overfitting}
\section{Applications of DL Handling Overfitting}
\section{Conclusions and Future Directions}
\appendix




\printglossaries

%\lstinputlisting[language=Python]{/Users/khalil/Desktop/Thesis/Code/BrainCancerDataset.py}
%\lstinputlisting[language=Python]{/Users/khalil/Desktop/Thesis/Code/PD_Dataset.py}

\newpage 
\bibliographystyle{plainyr}
\bibliography{ref}
\end{document}

